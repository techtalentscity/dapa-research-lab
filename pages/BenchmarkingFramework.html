<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DEPA Lab - Research</title>
    
    <!-- Fonts -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet">
    
    <!-- Styles - Note the ../ to go up one directory -->
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../styles/pagestyles.css">
    
    <!-- Load components script first -->
    <script src="../script/include.js"></script>
    <!-- Other scripts -->
    <script defer src="../scripts.js"></script>
</head>

<!-- Header -->

<body>
    <div id="header-container"></div>

<main>

<section class="page-banner">
  <div class="banner-content">
  </div>
</section>

<section class="academic-advisory">
    <div class="container">
        <h2 class="section-title">A Benchmarking Framework for Evaluating Cloud-Based and Open-Source Machine Learning Services</h2>
        
        <div class="content-single">
            <div class="content-card">
                <div class="card-icon">
                    <i class="fas fa-chart-line"></i>
                </div>
                <h3>Project Overview</h3>
                <p>This research presents AI/ML Bench Guard, a comprehensive benchmarking framework for evaluating cloud-based, LLM, and open-source machine learning services. The system conducts automated performance assessments across multiple providers, including AWS, Azure, GCP, and open-source alternatives, focusing on object detection, sentiment analysis, facial recognition, and activity recognition tasks.</p>
                <p>By implementing standardized testing protocols and continuous monitoring, AI/ML Bench Guard enables objective comparison of service performance, reliability, and cost-effectiveness while analyzing potential biases in model outputs. The framework features a public-facing dashboard for real-time performance visualization and historical trend analysis, promoting transparency in AI service evaluation.</p>
                <p>Results demonstrate significant improvements in service provider selection efficiency, operational cost reduction, and system reliability. This research contributes to the field by providing a standardized methodology for evaluating AI services and fostering trust through transparent performance metrics.</p>
            </div>
        </div>
    </div>
</section>

        
    </main>

   <!-- Footer -->
   <div id="footer-container"></div>
</body>
</html>
